---
title: "Question 2: Maximum likelihood"
output:
  pdf_document: 
    keep_tex: true
  html_document: default
editor_options: 
  markdown: 
    wrap: 72
tables: true
header-includes:
 \usepackage{float}
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```


```{r include=FALSE}
#  ---------------- Factory functions ----------------

#' Creates an alpha scheduler function for step size adjustment.
#'
#' @param factor Numeric. Reduction factor for alpha when no improvement 
#' is observed. Default is 0.1.
#'
#' @return A function that adjusts alpha based on function evaluations.
make_alpha_scheduler <- function(factor = 0.1) {
  
  scheduler <- function(alpha, val, newval) {
       
      if (val < newval) {
        # If val < newval (improvement), do nothing.
        return(alpha)
      } else {
        # Otherwise, reduce ´alpha´.
        return(alpha * factor)
      }
    }
  return(scheduler)
} 

#' Creates a log-likelihood function for logistic regression.
#'
#' @param x: A numeric vector of predictor values.
#' @param y: A numeric vector of binary response values (0 or 1).  
#'
#' @return: A function that computes the log-likelihood given a parameter vector TH.
make_llik <- function(x, y) {
  
  llik <- function(TH) {
    theta_0 <- TH[1]
    theta_1 <- TH[2]
    p <- 1 / (1 + exp(- theta_0 - theta_1 * x))
    log_likelihood <- sum(y * log(p) + (1 - y) * log(1 - p))
    
    return(log_likelihood)
  }
  
  return(llik)
}


#' Creates the gradient function of the log-likelihood function for logistic regression.
#'
#' @param x A numeric vector of predictor values.
#' @param y A numeric vector of binary response values (0 or 1).
#'
#' @return A function that computes the gradient given a parameter vector TH.
make_grad <- function(x, y) {
  
  grad <- function(TH) {
    theta_0 <- TH[1]
    theta_1 <- TH[2]
    
    p <- 1 / (1 + exp(-(theta_0 + theta_1 * x)))
    gradient <- c(sum(y - p), sum((y - p) * x))
    
    return(gradient)
  }
  
  return(grad)
}


#  ---------------- Steepest ascent ----------------


#' Performs steepest ascent optimization with optional step-size adaptation.
#'
#' @param f Function. The objective function to maximize.
#' @param g Function. The gradient function.
#' @param x0 Numeric. Initial point.
#' @param eps Numeric. Convergence threshold. Default is 1e-8.
#' @param alpha.0 Numeric. Initial step size. Default is 1.
#' @param alpha.scheduler Function. Step-size adjustment strategy. 
#' Default is NULL (constant step size).
#'
#' @return A data frame tracking the iteration history.
steepestasc <- function(
    f, 
    g, 
    x0, 
    eps = 1e-8, 
    alpha.0 = 1, 
    alpha.scheduler = NULL
 ){
  
  
  history <- data.frame(
    iter = numeric(0), 
    L = numeric(0),
    conv = numeric(0),
    alpha = numeric(0)
  )
  
  

  # If `alpha.scheduler` is NULL, use a constant value.
  if (is.null(alpha.scheduler)) {
    alpha.scheduler <- function(alpha, val, newval) alpha
  }
  
  # Define `step` function.
  step <- function(x, alpha) x + (alpha * g(x))
  
  # Perform a initial step and save it.
  x <- x0  
  alpha <- alpha.0 
  newx <- step(x, alpha) 
  conv <- sum((x - newx) * (x - newx))
  
  # Save initial step values.
  history[nrow(history) + 1, ] <- c(0, f(x), NA, NA)
  history[nrow(history) + 1, ] <- c(1, f(newx), conv, alpha)
  xt <- matrix(theta_init, ncol = length(x0)) 
  xt <- rbind(xt, as.vector(newx))
  
  i <- 1
  while(conv > eps){
    
    # Update `alpha`.
    alpha <- alpha.scheduler(alpha, f(x), f(newx))
    
    # Update `x` to previous iteration's new value.
    x <- newx
    newx <- step(x, alpha)
    
    # Update convergence criterion.
    conv <- sum((x-newx)*(x-newx))
    
    # Save current iteration.
    i <- i + 1
    history[nrow(history) + 1, ] <- c(i, f(newx), conv, alpha)
    xt <- rbind(xt, as.vector(newx))
    
  }
  
  
  return(list(xt=xt, history=history))
  
}

negate <- function(FUN) {
    negative_inverse <- function(x) - FUN(x)
    return(negative_inverse)
}

```



## Computing the ML estimator 

The ML estimator for the logistic regression model is computed by maximizing
the log-likelihood function given by

$$
\mathcal{L}(\mathbf{\theta}) = \sum_{i=1}^n \left\{y_i \log{ \left[\left(1 + \exp(-\theta_0 - \theta_1x_i)\right)^{-1}\right] + (1 - y_i) \log{\left[1 - (1 + \exp(\theta_0 + \theta_1x_i))^{-1} \right]}}     \right\}
$$

where $\left\{x_i, y_i \right\}_{i=1}^n$ is the observed dataset and $\mathbf{\theta} = \left(\theta_0, \theta_1 \right)^T$. Its gradient is,


$$
\nabla\mathcal{L} = \sum_{i=1}^n \left\{y_i - \frac{1}{1 + \exp(\theta_0 - \theta_1x_i)} \right\} \begin{pmatrix}
  1   \\
  x_i \\
\end{pmatrix}
$$

The steepest ascent method (implementation in the **Appendix**) updates $\theta$ 
iteratively in the direction of $\nabla\mathcal{L}$. That is,

$$
\theta_{t+1} = \theta_{t} + \alpha_{t}\nabla\mathcal{L}
$$

where $\alpha_{t}$ is the size of the step size at iteration $t$. The following
table summarizes the execution of this method for the maximization of 
$\mathcal{L}$ where only the first and last $3$ iteration values are shown.

```{r include=FALSE}
x <- c(0, 0, 0, 0.1, 0.1, 0.3, 0.3, 0.9, 0.9, 0.9)
y <- c(0, 0, 1, 0, 1, 1, 1, 0, 1, 1)

# Define the log-likelihood and gradient functions
llik <- make_llik(x, y)
grad <- make_grad(x, y)

# Set an initial guess for theta
theta_init <- c(-0.2, 1)

# Perform optimization
result <- steepestasc(
  f = llik, 
  g = grad, 
  x0 = theta_init, 
  eps = 1e-8, 
  alpha.0 = 1, 
  alpha.scheduler = make_alpha_scheduler(0.5)
)
```

```{r echo=FALSE}
summary <- cbind(result$history, result$xt)
theta_names <- c("$\\theta_0$", "$\\theta_1$")
newnames <- c(names(summary)[1:4], as.vector(theta_names))
names(summary) <- newnames
summary <- round(summary, 5)
summary <- rbind(head(summary, 3), tail(summary, 3))
knitr::kable(
  summary,
  row.names = FALSE,
  caption = "Steepest ascent algorithm for $(\\theta_0, \\theta_1) = (-0.2, 1)$. Last two columns represent $\\theta_0$ and $\\theta_1$, respectively. Only the first and last $3$ iteration values are shown"
)
```

The algorithm was executed using a step-size reduction strategy. In particular,
$\alpha$ is halved (i.e., reduced by a factor of 2) whenever the the 
log-likelihood decreases between iterations. For our case, this happens between 
iteration $0$ and $1$, leading to $\alpha = 0.5$ in iteration $2$.

To further illustrate this strategy, the following table presents the execution 
results when using a reduction factor of $4$ (i.e., alpha is reduced to a 
quarter of its value).

```{r include=FALSE}
# Perform optimization
result <- steepestasc(
  f = llik, 
  g = grad, 
  x0 = theta_init, 
  eps = 1e-8, 
  alpha.0 = 1, 
  alpha.scheduler = make_alpha_scheduler(0.25)
)
```

```{r echo=FALSE}
summary <- cbind(result$history, result$xt)
theta_names <- c("$\\theta_0$", "$\\theta_1$")
newnames <- c(names(summary)[1:4], as.vector(theta_names))
names(summary) <- newnames
summary <- round(summary, 5)
summary <- rbind(head(summary, 3), tail(summary, 3))
knitr::kable(
  summary,
  row.names = FALSE,
  caption = "Steepest ascent algorithm for $(\\theta_0, \\theta_1) = (-0.2, 1)$ and reduction factor of 4. Only the first and last $3$ iteration values are shown."
)
```
## Comparisson with BFGS and Nelder-Mead algorithms

Next, instead of our own implementation of the steepest ascent optimizer, 
we use the function `optim` with both the BFGS and Nelder-Mead algorithm.
The summary for BFGS is

```{r include=FALSE}
res_BFGS <- optim(c(-2, 1), fn = negate(llik), gr = negate(grad), method = "BFGS")
res_NM <- optim(c(-2, 1), fn = negate(llik), gr = negate(grad), method = "Nelder-Mead")
```

```{r echo=FALSE}
summary <- as.data.frame(matrix(c(-res_BFGS$value, as.vector(res_BFGS$counts), res_BFGS$par), nrow = 1))
names(summary) <- c("L", "FunctionCalls", "GradientCalls", "$\\theta_0$", "$\\theta_1$")

knitr::kable(
  round(summary, 5),
  caption = "Results for BFGS"
)

```


Similarly, for Nelder-Mead,

```{r echo=FALSE}
summary <- as.data.frame(matrix(c(-res_NM$value, as.vector(res_NM$counts), res_NM$par), nrow = 1))
names(summary) <- c("L", "FunctionCalls", "GradientCalls", "$\\theta_0$", "$\\theta_1$")

knitr::kable(
  round(summary, 5),
  caption = "Results for Nelder-Mead"
)

```

As side note, the function `optim` minimizes the given objective function (in this
case the log-likelihood). Since we are looking for the maximum, our objective
was negated (i.e., $-\mathcal{L}$) in advance.

Among the executed algorithms, BFGS is the fastest, reaching the maximum in 
just $15$ function calls, compared to $51$ for Nelder-Mead. Both BFGS and 
Nelder-Mead converge to a solution similar to that of steepest ascent, differing 
only by a magnitude of $10^{-3}$ in both $\theta_0$ and $\theta_1$.


## Using the `glm` function

Here we proceed by using the function `glm` that allows us to perform
a logistic function directly. The summary for the output model is the following

```{r echo=TRUE}
logistic <- glm(y ~ x, data = data.frame(x=x, y=y), family = "binomial")
summary(logistic)
```

Notice that the coefficient values match those obtained from the optimization algorithms.






## Apendix

```r
#  ---------------- Factory functions ----------------

#' Creates an alpha scheduler function for step size adjustment.
#'
#' @param factor Numeric. Reduction factor for alpha when no improvement 
#' is observed. Default is 0.1.
#'
#' @return A function that adjusts alpha based on function evaluations.
make_alpha_scheduler <- function(factor = 0.1) {
  
  scheduler <- function(alpha, val, newval) {
       
      if (val < newval) {
        # If val < newval (improvement), do nothing.
        return(alpha)
      } else {
        # Otherwise, reduce ´alpha´.
        return(alpha * factor)
      }
    }
  return(scheduler)
} 

#' Creates a log-likelihood function for logistic regression.
#'
#' @param x: A numeric vector of predictor values.
#' @param y: A numeric vector of binary response values (0 or 1).  
#'
#' @return: A function that computes the log-likelihood given a parameter vector TH.
make_llik <- function(x, y) {
  
  llik <- function(TH) {
    theta_0 <- TH[1]
    theta_1 <- TH[2]
    p <- 1 / (1 + exp(- theta_0 - theta_1 * x))
    log_likelihood <- sum(y * log(p) + (1 - y) * log(1 - p))
    
    return(log_likelihood)
  }
  
  return(llik)
}


#' Creates the gradient function of the log-likelihood function for logistic regression.
#'
#' @param x A numeric vector of predictor values.
#' @param y A numeric vector of binary response values (0 or 1).
#'
#' @return A function that computes the gradient given a parameter vector TH.
make_grad <- function(x, y) {
  
  grad <- function(TH) {
    theta_0 <- TH[1]
    theta_1 <- TH[2]
    
    p <- 1 / (1 + exp(-(theta_0 + theta_1 * x)))
    gradient <- c(sum(y - p), sum((y - p) * x))
    
    return(gradient)
  }
  
  return(grad)
}


#' Negate a Function's Output  
#'  
#' This function takes a single-argument function and returns a new function  
#' that computes the negative of the original function's output.  
#'  
#' @param FUN A function that takes a single numeric argument.  
#' @return A function that returns the negated output of `FUN`.  
#' @examples
negate <- function(FUN) {
    negative_inverse <- function(x) - FUN(x)
    return(negative_inverse)
}


#  ---------------- Steepest ascent ----------------


#' Performs steepest ascent optimization with optional step-size adaptation.
#'
#' @param f Function. The objective function to maximize.
#' @param g Function. The gradient function.
#' @param x0 Numeric. Initial point.
#' @param eps Numeric. Convergence threshold. Default is 1e-8.
#' @param alpha.0 Numeric. Initial step size. Default is 1.
#' @param alpha.scheduler Function. Step-size adjustment strategy. 
#' Default is NULL (constant step size).
#'
#' @return A data frame tracking the iteration history.
steepestasc <- function(
    f, 
    g, 
    x0, 
    eps = 1e-8, 
    alpha.0 = 1, 
    alpha.scheduler = NULL
 ){
  
  history <- data.frame(
    iter = numeric(0), 
    L = numeric(0),
    conv = numeric(0),
    alpha = numeric(0)
  )

  # If `alpha.scheduler` is NULL, use a constant value.
  if (is.null(alpha.scheduler)) {
    alpha.scheduler <- function(alpha, val, newval) alpha
  }
  
  # Define `step` function.
  step <- function(x, alpha) x + (alpha * g(x))
  
  # Perform a initial step and save it.
  x <- x0  
  alpha <- alpha.0 
  newx <- step(x, alpha) 
  conv <- sum((x - newx) * (x - newx))
  
  history[nrow(history) + 1, ] <- c(0, f(x), NA, NA)
  history[nrow(history) + 1, ] <- c(1, f(newx), conv, alpha)
  
  i <- 1
  while(conv > eps){
    
    # Update `alpha`.
    alpha <- alpha.scheduler(alpha, f(x), f(newx))
    
    # Update `x` to previous iteration's new value.
    x <- newx
    newx <- step(x, alpha)
    
    # Update convergence criterion.
    conv <- sum((x-newx)*(x-newx))
    
    # Save current iteration.
    i <- i + 1
    history[nrow(history) + 1, ] <- c(i, f(newx), conv, alpha)
    
  }
  
  return(list(x=newx, history=history))
  
}

# Define dataset
x <- c(0, 0, 0, 0.1, 0.1, 0.3, 0.3, 0.9, 0.9, 0.9)
y <- c(0, 0, 1, 0, 1, 1, 1, 0, 1, 1)

# Define the log-likelihood and gradient functions
llik <- make_llik(x, y)
grad <- make_grad(x, y)

# Set an initial guess for theta
theta_init <- c(-0.2, 1)

# Perform optimization
result <- steepestasc(
  f = llik, 
  g = grad, 
  x0 = theta_init, 
  eps = 1e-3, 
  alpha.0 = 1, 
  alpha.scheduler = make_alpha_scheduler(0.5)
)

# BFGS and Nelder-Mead
res_BFGS <- optim(c(-2, 1), fn = negate(llik), gr = negate(grad), method = "BFGS")
res_NM <- optim(c(-2, 1), fn = negate(llik), gr = negate(grad), method = "Nelder-Mead")

# glm
logistic <- glm(y ~ x, data = data.frame(x=x, y=y), family = "binomial")

```



